[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Dave is a Master’s student completing his MSc in Applied Data Science at UChicago and consults on business analytics, technology project management, and data science.\nWhen not geeking out on technology and data, Dave enjoys spending time on the basketball court and shaping a tiny mind at home."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nUniversity of Chicago | Chicago, IL MSc in Applied Data Science | 2025 - present\nSimon Fraser University | Vancouver, BC BBA Joint Major in Computer Science and Business Administration | 2015"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nIndependent Consulting | Consultant | 2023 - present\nRelocated to US and started a family!\nTELUS | Senior Business Analyst | 2018 - 2022\nTELUS | Business Analyst | 2015 - 2018"
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html",
    "href": "projects/machine_learning/netflix_part2_eda.html",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "",
    "text": "Photo by Joakim Nådell on Unsplash\nThis is Part 2 of 4 in a project to build an explainable machine learning model to predict Netflix customer churn.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html#overview",
    "href": "projects/machine_learning/netflix_part2_eda.html#overview",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "Overview",
    "text": "Overview\nIn this section, we approach the EDA and data understanding by:\n\nApplying data engineering through a binning strategy as an alternate means to interpret certain features and for possible use in downstream models.\nVisually inspect distributions of categorical and numerical features in relation to churn using plotly\nStatistically examine their relationship with churn using pandas and statsmodels\nUnderstand collinearity between features using pandas and statsmodels\nVisually understand two high importance features and their relationship with churn using plotly\n\nThese steps allow us to reveal patterns, identify data quality issues, uncover relationships between varaibles, and help validate assumptions. Altogether they help form the foundation from which our modelling decisions will be made.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html#imports-and-data-load",
    "href": "projects/machine_learning/netflix_part2_eda.html#imports-and-data-load",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "Imports and data load",
    "text": "Imports and data load\n\n# Import libraries\nimport plotly.express as px\nfrom sklearn.preprocessing import OneHotEncoder\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport os\n\n\n# Load the data\nproject_path = os.getcwd()\ndf = pd.read_csv(project_path + \"/Data/netflix_customer_churn.csv\")\n\n\n# Set the default template for plotly\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\n\n\n# Set plotly render to work with Quarto\npio.renderers.default = 'notebook_connected' # or 'notebook_connected'",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html#data-description-and-engineering",
    "href": "projects/machine_learning/netflix_part2_eda.html#data-description-and-engineering",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "Data description and engineering",
    "text": "Data description and engineering\nIn this dataset with 5,000 observations, a unit of observation represents a single subscriber’s information across: age, gender, subscription_type, watch_hours, last_login_days, region, device, monthly_fee, payment_method, number_of_profiles, avg_watch_time_per_day, and favorite_genre. Additionally, we have churned representing their churned status. Thanks to the sensibily named variables, for brevity we will forgo formal variable definitions.\nGiven the synthetic nature of the data and no other means to validate, a reasonable assumption we work with is that the usage data within this dataset is a weekly snapshot. For example, the median for watch_hours is 8 hours (a plausible amount of weekly watch hours) and no subscribers have watch hours over 110 hours (well within the 168 hours available in a 7 day week).\nThe dataset is clean with no missing values, which we can attribute to the great work of our hypothetical data engineering team.\nBased on the observed characteristics of each feature, we proactively define our numerical and categorical columns so that they are treated as such in our analysis and modelling downstream. For example, monthly_fee, while technically numerical, is more meaningfully treated as a categorical given it contains only three possible values (8.99, 13.99, or 17.99).\n\n# Create column name lists for reference\ncategorical_cols = ['gender', 'subscription_type', 'region', 'device', 'payment_method', 'favorite_genre', 'monthly_fee',  'number_of_profiles']\nnumerical_cols = ['age', 'watch_hours', 'last_login_days', 'avg_watch_time_per_day']\n\n\ndf.head().T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\ncustomer_id\na9b75100-82a8-427a-a208-72f24052884a\n49a5dfd9-7e69-4022-a6ad-0a1b9767fb5b\n4d71f6ce-fca9-4ff7-8afa-197ac24de14b\nd3c72c38-631b-4f9e-8a0e-de103cad1a7d\n4e265c34-103a-4dbb-9553-76c9aa47e946\n\n\nage\n51\n47\n27\n53\n56\n\n\ngender\nOther\nOther\nFemale\nOther\nOther\n\n\nsubscription_type\nBasic\nStandard\nStandard\nPremium\nStandard\n\n\nwatch_hours\n14.73\n0.7\n16.32\n4.51\n1.89\n\n\nlast_login_days\n29\n19\n10\n12\n13\n\n\nregion\nAfrica\nEurope\nAsia\nOceania\nAfrica\n\n\ndevice\nTV\nMobile\nTV\nTV\nMobile\n\n\nmonthly_fee\n8.99\n13.99\n13.99\n17.99\n13.99\n\n\nchurned\n1\n1\n0\n1\n1\n\n\npayment_method\nGift Card\nGift Card\nCrypto\nCrypto\nCrypto\n\n\nnumber_of_profiles\n1\n5\n2\n2\n2\n\n\navg_watch_time_per_day\n0.49\n0.03\n1.48\n0.35\n0.13\n\n\nfavorite_genre\nAction\nSci-Fi\nDrama\nHorror\nAction\n\n\n\n\n\n\n\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nage\n5000.0\n43.84740\n15.501128\n18.00\n30.0000\n44.00\n58.00\n70.00\n\n\nwatch_hours\n5000.0\n11.64945\n12.014654\n0.01\n3.3375\n8.00\n16.03\n110.40\n\n\nlast_login_days\n5000.0\n30.08980\n17.536078\n0.00\n15.0000\n30.00\n45.00\n60.00\n\n\nmonthly_fee\n5000.0\n13.68340\n3.692062\n8.99\n8.9900\n13.99\n17.99\n17.99\n\n\nchurned\n5000.0\n0.50300\n0.500041\n0.00\n0.0000\n1.00\n1.00\n1.00\n\n\nnumber_of_profiles\n5000.0\n3.02440\n1.415841\n1.00\n2.0000\n3.00\n4.00\n5.00\n\n\navg_watch_time_per_day\n5000.0\n0.87480\n2.619824\n0.00\n0.1100\n0.29\n0.72\n98.42\n\n\n\n\n\n\n\nWe additionally define three new categoricals that are binned versions of age, last_login_days, and watch_hours which align with groupings typical for understanding demographics and engagement.\n\n# Create bins for age group (ie. age)\ndf['age_grp'] = pd.cut(df['age'], bins=[0, 18, 24, 34, 44, 54, 64, np.inf],\n                        labels=['0-17', '18–24', '25–34', '35–44', '45–54', '55–64', '65+'])\n\n# Create bins for last login group (ie. last_login_days)\ndf['last_login_grp'] = pd.cut(df['last_login_days'], bins=[0, 7, 14, 30, np.inf],\n                        labels=['0-7', '7-14', '14-30', '30+'])\n\n# Create bins for engagement level (ie. watch_hours)\ndf['engagement_grp'] = pd.cut(df['watch_hours'], bins=[0, 1, 5, 10, 20, 200],\n                        labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html#churn-rate-overview",
    "href": "projects/machine_learning/netflix_part2_eda.html#churn-rate-overview",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "Churn rate overview",
    "text": "Churn rate overview\nAcross categorical and numeric features, the key highlights can be summarized as follows for subscribers who churn versus those who do not. Churned subscribers:\n\nAre more likely to be on a Basic plan (40% vs. 25%, see subscription_type, monthly_fee)\nUse Crypto and Gift Card payments more often (50% vs. 32%, see payment_method)\nAre more typically have 3 or less profiles created (66% vs. 40%, see number_of_profiles)\nFrequently have not logged in for &gt; 30 days (73% vs. 25%, see last_login_grp, last_login_days)\nHave low or very low engagement with &lt; 5 watch hours (60% vs. 11%, see engagement_grp, watch_hours)\n\nThis indicates that these features together can likely provide a meaningful ability to predict churn.\n\nSetup with helper functions\nTo expediently produce visual plots of both our categorical and numerical features, their distributions, and their relationship against our target variable churned, we define two helper functions plot_category and plot_numeric that will allow us to loop through features and create appropriate plots automatically.\n\n# Update column name lists for reference\ncategorical_cols = ['gender', 'subscription_type', 'region', 'device', 'payment_method', 'favorite_genre', 'monthly_fee',  'number_of_profiles']\ncategorical_cols = categorical_cols + ['age_grp', 'last_login_grp', 'engagement_grp']\n\n\n# Make all categoricals of category data type\nfor category in categorical_cols:\n    df[category] = df[category].astype(\"category\")\n\nDefine function for plot of categorical variables against target\n\ndef plot_category(df, category, target):\n    # Create a churn rate comparison between groups - histogram for categorical (no curve option in express)\n    fig = px.histogram(df, x=category, color=target, histnorm=\"percent\",\n                    category_orders={category: df[category].cat.categories},\n                    barmode='group', opacity=0.75,\n                    title=f\"{category} Distribution\",\n                    template=\"plotly_dark\"          \n                    )\n    fig.update_layout(title_subtitle_text=f\"by {target}, n=5000\")\n    fig.show()\n\nDefine function for plot of continuous variables against target\n\ndef plot_numeric(df, numeric, target):\n    # Create a churn rate comparison between groups - violin plot for continuous\n    fig = px.violin(df, x=numeric, color=target,\n                    violinmode = 'overlay', # overlay or group\n                    title=f\"{numeric} Distribution\",\n                    template=\"plotly_dark\")\n    fig.update_layout(title_subtitle_text=f\"by {target}, n=5000\")\n    fig.show()\n\n\n\nEDA for categoricals\n\nfor category in categorical_cols:\n    plot_category(df, category, \"churned\")\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\nEDA for numeric\n\nfor numeric in numerical_cols:\n    plot_numeric(df, numeric, \"churned\")",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html#feature-explanatory-power",
    "href": "projects/machine_learning/netflix_part2_eda.html#feature-explanatory-power",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "Feature explanatory power",
    "text": "Feature explanatory power\nTo understand each feature’s explanatory power through more statistical means, we additionally examine both 1) pearson correlation coefficients of each feature against target and 2) the p-value of each feature from a logistic regression model.\nFeatures correlated with churn:\n\nlast_login_days and watch_hours have high correlation (0.47 and -0.47 respectively)\navg_watch_time_per_day has moderate correlation (-0.27)\nnumber_of_profiles, subscription_type, payment_method have minor correlation (between |0.05| and |0.12| depending on category)\n\nFeatures with lowest p-values (p &lt; 0.05 indicating regression coefficient significantly different than 0):\n\nnumber_of_profiles\nlast_login_days\nwatch_hours\npayment_method\navg_watch_time_per_day\n\nIn both approaches, we can validate that the same features we found important in our visual analysis is further corroborated as significant through verification with statistical methods. Notably, subscription_type was not found to be significant and this is reasonable as the correlation was not strong to begin with. If our features were standard scaled, we could also use the coefficients in the logistic regression as a means to understand relative importance.\nNote that these approaches will only capture linear relationships as we don’t have any priors to suggest otherwise. However, any non-linear relationship should be quantified through other approaches such Distance Correlation or Rank-based Correlation.\n\nEncode categoricals for analysis\nWe will first make dummy variables for the categorical features, using pd.get_dummies for expedience.\n\n# Revert back to original categoricals before engineered categories \ncategorical_cols = ['gender', 'subscription_type', 'region', 'device', 'payment_method', 'favorite_genre', 'monthly_fee',  'number_of_profiles']\n\n# Create temporary new dataset with categorical values encoded\ntemp_df = df[numerical_cols]\nfor category in categorical_cols:\n    dummy_df = pd.get_dummies(df[category], prefix=category, dtype=int, drop_first=True)\n    temp_df = temp_df.merge(dummy_df, left_index=True, right_index=True)\n\n\n\nCorrelation with target\n\n# Calculate the correlation\ntarget_corr = temp_df.corrwith(df['churned']).to_frame(name=\"coef\")\n\n# Plot the correlation\nfig = px.bar(target_corr, x='coef', color='coef',\n            title=f\"Feature correlation with target\")\nfig.update_layout(\n    title_subtitle_text=f\"No correlation = 0 while proximity to -1 (blue) or +1 (yellow) indicates strength &lt;br&gt;of linear relationship\",\n    yaxis={'categoryorder':'total ascending'})   \nfig.show()\n\n                            \n                                            \n\n\n\n\nRegression coefficients\nWe apply logistic regression here as our target is a discrete dependent variable. Note that while typically EDA can be done before train/test split to understand relationship, the criteria for feature selection should be done after the split to mitigate risk of data leakage.\n\n# Add the target to our encoded df\ntarget = df['churned']\npredictors = sm.add_constant(temp_df)\n\n# Create regression\nlogit_model = sm.Logit(target, predictors).fit()\nprint(logit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.216549\n         Iterations 11\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                churned   No. Observations:                 5000\nModel:                          Logit   Df Residuals:                     4968\nMethod:                           MLE   Df Model:                           31\nDate:                Mon, 15 Sep 2025   Pseudo R-squ.:                  0.6876\nTime:                        12:42:47   Log-Likelihood:                -1082.7\nconverged:                       True   LL-Null:                       -3465.6\nCovariance Type:            nonrobust   LLR p-value:                     0.000\n==============================================================================================\n                                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------------\nconst                          3.1403      0.259     12.119      0.000       2.632       3.648\nage                            0.0005      0.004      0.140      0.889      -0.007       0.008\nwatch_hours                   -0.2750      0.020    -13.991      0.000      -0.314      -0.237\nlast_login_days                0.1207      0.007     16.416      0.000       0.106       0.135\navg_watch_time_per_day        -4.0225      0.503     -7.990      0.000      -5.009      -3.036\ngender_Male                   -0.2388      0.135     -1.765      0.078      -0.504       0.026\ngender_Other                  -0.1271      0.135     -0.943      0.346      -0.391       0.137\nsubscription_type_Premium     -1.2572   5.87e+06  -2.14e-07      1.000   -1.15e+07    1.15e+07\nsubscription_type_Standard    -1.2211        nan        nan        nan         nan         nan\nregion_Asia                   -0.0590      0.121     -0.487      0.626      -0.296       0.178\nregion_Europe                 -0.0035      0.115     -0.030      0.976      -0.228       0.221\nregion_North America           0.0822      0.150      0.548      0.583      -0.212       0.376\nregion_Oceania                 0.0285      0.148      0.193      0.847      -0.261       0.319\nregion_South America           0.2113      0.153      1.383      0.167      -0.088       0.511\ndevice_Laptop                  0.2390      0.164      1.458      0.145      -0.082       0.560\ndevice_Mobile                  0.2693      0.167      1.614      0.107      -0.058       0.596\ndevice_TV                      0.1371      0.162      0.845      0.398      -0.181       0.455\ndevice_Tablet                  0.1214      0.159      0.764      0.445      -0.190       0.433\npayment_method_Crypto          1.8651      0.185     10.109      0.000       1.504       2.227\npayment_method_Debit Card     -0.2070      0.170     -1.217      0.224      -0.540       0.126\npayment_method_Gift Card       1.6976      0.187      9.089      0.000       1.332       2.064\npayment_method_PayPal         -0.0816      0.169     -0.482      0.630      -0.414       0.251\nfavorite_genre_Comedy          0.0401      0.208      0.193      0.847      -0.367       0.447\nfavorite_genre_Documentary    -0.1583      0.206     -0.769      0.442      -0.562       0.245\nfavorite_genre_Drama           0.1408      0.200      0.706      0.480      -0.250       0.532\nfavorite_genre_Horror         -0.1286      0.202     -0.636      0.525      -0.525       0.268\nfavorite_genre_Romance        -0.1736      0.203     -0.853      0.394      -0.572       0.225\nfavorite_genre_Sci-Fi         -0.2085      0.200     -1.043      0.297      -0.600       0.183\nmonthly_fee_13.99             -1.2211        nan        nan        nan         nan         nan\nmonthly_fee_17.99             -1.2572   1.05e+07  -1.19e-07      1.000   -2.06e+07    2.06e+07\nnumber_of_profiles_2          -0.6596      0.170     -3.873      0.000      -0.993      -0.326\nnumber_of_profiles_3          -0.6856      0.161     -4.246      0.000      -1.002      -0.369\nnumber_of_profiles_4          -3.1340      0.181    -17.343      0.000      -3.488      -2.780\nnumber_of_profiles_5          -3.0859      0.176    -17.546      0.000      -3.431      -2.741\n==============================================================================================\n\nPossibly complete quasi-separation: A fraction 0.14 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n\n# Plot p-values\npval_df = pd.DataFrame(logit_model.pvalues, columns=['pval']).sort_values(by='pval', ascending=True)\nfig = px.imshow(pval_df, text_auto=True, aspect='auto', height=len(pval_df)*25, \n                color_continuous_scale=px.colors.sequential.Darkmint_r,\n                title=f\"Feature signficance based on regression analysis\"       \n                )\nfig.update_layout(\n    title_subtitle_text=f\"Significant features = darker (p-value &lt; 0.05)\",\n    yaxis={'categoryorder':'total ascending'}\n    )",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html#collinearity",
    "href": "projects/machine_learning/netflix_part2_eda.html#collinearity",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "Collinearity",
    "text": "Collinearity\nTo fully understand the independent contribution of each feature, we may want to identify any collinearity between features.\nThis could allow us identify and remove a highly collinear feature to simplify the models we build and make coefficients easier to interpret. However, if the primary objective is to make accurate predictions, the model might still perform well with collinear features.\nWe will examine collinearity with two methods: 1) correlation matrix and 2) variance inflation factor (VIF).\nLike our previous section on feature explanatory power, typically this should be done after train/test split to mitigate risk of data leakage.\n\nCorrelation matrix\nThe correlation matrix indicates the following features with collinearity:\n\nsubscription_type and monthly_fee (1.0): The perfect correlation is expected since the monthly fee follows the type of subscription plan\navg_watch_time_per_day and watch_hours (0.35): A moderate correlation is reasonable as average watch time is in part driven by the number of hours watched; higher watch hours drives higher average watch time provided the same number of days watched\navg_watch_time_per_day and last_login_days (-0.34): A moderate negative correlation is somewhat unexpected and likely due to issues with synthetic data generation noted below\n\nRemaining weaker correlations are generally between one-hot-encoded categorical features where some within-feature category correlation is expected.\n\n# Create the correlation matrix\ncorr_df = temp_df.corr(method='pearson')\n\n\n# Show correlation matrix in a heatmap\nfig = px.imshow(corr_df, text_auto=True,\n                title=f\"Correlation matrix\"       \n                )\nfig.update_layout(title_subtitle_text=f\"Highly correlated features = yellow and blue\")                \nfig.show()\n\n                            \n                                            \n\n\n\nAn auxillary view to illustrate the relationship between watch_hours, avg_watch_hours_per_day, and last_login_days is shown below.\nSurprisingly, the linear relationship is weaker for greater lapses in days since last login. This indicates that despite having presumably less days to watch, subscribers who login infrequently are able notch high volumes of watch hours while somehow only watching very few hours per day. This is at odds with expectation since with fewer watch days, watch hours per day should increase but instead remains constant here. This illustrates potential pitfalls with synthetic data.\n\n# Show scatterplot of `watch_hours`, `avg_watch_hours_per_day`, and `last_login_days`\nfig = px.scatter(df, x='watch_hours', y='avg_watch_time_per_day', color='last_login_days',\n                 title=f\"Scatter plot of avg_watch_hours_per_day vs watch_hours\")\nfig.update_layout(title_subtitle_text=f\"For greater lapses in login days, changes to watch hours increasingly has no affect &lt;br&gt;on daily watch hours despite an expecation for the opposite.\") \nfig.show()\n\n                            \n                                            \n\n\n\n\nVariance inflation factor\nVariance Inflation Factor (VIF) provides an alternate view in quantifying the extent that the variance of a regression coefficient is increased due to multicollearity. This is done by running multiple regressions with each independent variable as the target while the remainder serve as predictors. The resulting R-squared value from each of these auxiliary regressions is then used to calculate the VIF for the specific variable.\nThe VIF results below additionally identifies age as having high collinearity with other exogenous variables. Additionally, features that have inf values (or no bar) are also highly collinear - these collinear features align with our previous observations in the correlation matrix.\nFrom here if we want to know what age is collinear with, we can further specify an auxiliary regression with age as an target (and all other features as predictors) to identify significant features. However we forgo this as age was previously shown to not be a statistically significant feature.\n\n# Create function to calculate the VIF using statsmodel\ndef show_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return(vif_data.sort_values(by=\"VIF\", ascending=False))\n\n\n# Plot the VIF\nvif_df = show_vif(temp_df)\nfig = px.bar(vif_df, x='VIF', y='feature', color='VIF',\n            title=f\"Variance inflation factor\"       \n            )\nfig.update_layout(\n    title_subtitle_text=f\"High multicollinearity = yellow (VIF &gt; 5)\",\n    yaxis={'categoryorder':'total ascending'}\n    )   \nfig.show()",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part2_eda.html#multi-dimensional-relationships",
    "href": "projects/machine_learning/netflix_part2_eda.html#multi-dimensional-relationships",
    "title": "Netflix Part 2: Data Understanding & EDA Insights",
    "section": "Multi-dimensional relationships",
    "text": "Multi-dimensional relationships\nTo summarize some of the high importance features we found during EDA in a compact visualization, we can make use of the parallel categories diagram (also known as parallel sets or alluvial diagram).\nThe diagram is particulary useful for multi-dimensional categorical data sets, where each categorical variable in the data set is represented by a column of rectangles and each rectangle is a discrete value within that variable. The relative heights of each rectangle represents the relative frequency of that discrete value within the variable.\nIn the parallel categories diagram below, we show last_login, engagement_grp, and churned together to show how typical subscribers that churn (white highlight) can be associated both with long lapses since last login time and low engagement levels (with watch hours of 5 hours or less).\n\n# Plot the relationship between engagement_grp, last_login_grp, and churned\nnf_palette = ['#B20710','#F5F5F1']\ndims = ['last_login_grp', 'engagement_grp', 'churned']\nfig = px.parallel_categories(df, dimensions=dims, color='churned',\n                             color_continuous_scale=nf_palette,\n                             color_continuous_midpoint=0.5,\n                             height=600,\n                             labels={'last_login_grp': 'Last Login (days)', \n                                     'engagement_grp': 'Engagement (hours watched)', \n                                     'churned': 'Churned'},\n                             title=f\"Relationship between engagement metrics and churn\")\nfig.update_layout(\n    title_subtitle_text=f\"Churn is driven by long lapses since last login and low engagement levels\")\nfig.update_traces(line_shape='hspline', selector=dict(type='parcats'))\nfig.update_layout(xaxis=dict(showgrid=False))\nfig.show()",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 2: Data Understanding & EDA Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part3_mlflow.html",
    "href": "projects/machine_learning/netflix_part3_mlflow.html",
    "title": "Netflix Part 3: Modeling Strategy & Experiments",
    "section": "",
    "text": "Photo by SLNC on Unsplash\nThis is Part 3 of 4 in a project to build an explainable machine learning model to predict Netflix customer churn.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 3: Modeling Strategy & Experiments"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part3_mlflow.html#overview",
    "href": "projects/machine_learning/netflix_part3_mlflow.html#overview",
    "title": "Netflix Part 3: Modeling Strategy & Experiments",
    "section": "Overview",
    "text": "Overview\nIn this section, we approach the modeling strategy and experiments as follows:\n\nSetup logging for our model experiments with mlflow (a popular MLOps framework for reproducible experiments and model lifecycle management)\nApply train/test split to maintain a hold-out data set for validation\nEvaluate baseline models across four machine learning model types using scikit-learn\nSummarize the results and discuss performance trade-offs\nExamine key artifacts for the selected model\n\nThese steps allow us to organize our experiments for reproducibility with models logged and ready for production. Additionally, starting with baseline models follows the concept of parsimony, giving us a point of reference to determine if more complex and resource-intensive models are required.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 3: Modeling Strategy & Experiments"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part3_mlflow.html#imports-and-data-load",
    "href": "projects/machine_learning/netflix_part3_mlflow.html#imports-and-data-load",
    "title": "Netflix Part 3: Modeling Strategy & Experiments",
    "section": "Imports and data load",
    "text": "Imports and data load\n\n# Import numerical processing\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Import mlflow\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.models import infer_signature\n\n# Import machine learning\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n\n# Load the data\nproject_path = os.getcwd()\ndf = pd.read_csv(project_path + \"/Data/netflix_customer_churn.csv\")\n\nTo prepare the data for modeling, we follow the same numerical and categorical delineation we identified during EDA. Additionally, we need to change data types to be float (for numerical features) and string (for categorical features) to ensure compatibility with mlflow’s strict enforcement of model signatures/schema.\n\n# Clean the data with expedience and for mlflow compatibility\n\n# Create column name lists for reference\nnumerical_features = ['age', 'watch_hours', 'last_login_days', 'avg_watch_time_per_day']\ncategorical_features = ['gender', 'subscription_type', 'region', 'device', 'payment_method', 'favorite_genre', 'monthly_fee',  'number_of_profiles']\ntarget = ['churned']\n\n# Make all categoricals of string data type\nfor category in categorical_features:\n    df[category] = df[category].astype(\"string\")\n\n# Make all numerical of float data type (be wary of precision loss for large integers)\nfor numerical in numerical_features:\n    df[numerical] = df[numerical].astype(\"float\")\n\n# Make target of float data type (be wary of precision loss for large integers)\n#df[target] = df[target].astype(\"float\")\n\n# Drop customer_id\ntry:\n    df = df.drop(\"customer_id\", axis=1)\nexcept KeyError:\n    print(\"Already dropped customer_id\")",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 3: Modeling Strategy & Experiments"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part3_mlflow.html#baseline-model-comparison-in-mlflow",
    "href": "projects/machine_learning/netflix_part3_mlflow.html#baseline-model-comparison-in-mlflow",
    "title": "Netflix Part 3: Modeling Strategy & Experiments",
    "section": "Baseline model comparison in mlflow",
    "text": "Baseline model comparison in mlflow\nTo enable tracking our model expriments in mlflow, we first setup a connection to our local server session. This can be setup with a local installation of mlflow and running mlflow ui from the command line.\nAlternatively for MLOps in the cloud, we could point to a MLflow Tracking Server hosted through any cloud platform provider, such as Databricks.\nOnce the connection is established, we setup an experiment specific to our prediction use case so that all model experiment details (ie. metrics, models, artifacts, etc.) can be tracked and organized under this experiment.\n\nSetup session\n\n# Connect MLflow session to local server\nmlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n\n\n\nSetup experiment\n\n# Run a test experiment\nmlflow.set_experiment(\"Netflix Churn Prediction\")\n\nwith mlflow.start_run():\n    mlflow.log_metric(\"foo\", 1)\n    mlflow.log_metric(\"bar\", 2)\n\n🏃 View run dapper-turtle-416 at: http://127.0.0.1:5000/#/experiments/888675540034408296/runs/f50542b4fc67475fa325780ed9184b60\n🧪 View experiment at: http://127.0.0.1:5000/#/experiments/888675540034408296\n\n\n\n\nSplit the data\nWe will apply a standard 80/20 split of our data between training and testing.\nFor more effective hyperparameter tuning and model selection, we could further apply cross-validation on the training data during the experimentation process. The test set would then be set aside for an unbiased final validation against candidate models retrained against the entire training set. However we proceed without due to the high performance of baseline models, removing the need for additional hyperparameter tuning.\n\n# Split into 80% training and 20% testing\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_df.shape, test_df.shape\n\n((4000, 13), (1000, 13))\n\n\n\n# Separate the target column for the training set\ntrain_dataset = mlflow.data.from_pandas(train_df, name=\"train\")\nX_train = train_dataset.df.drop([\"churned\"], axis=1)\ny_train = train_dataset.df[[\"churned\"]].values.ravel()\n\nX_train.shape, y_train.shape\n\n((4000, 12), (4000,))\n\n\n\n# Separate the target column for the testing set\ntest_dataset = mlflow.data.from_pandas(test_df, name=\"test\")\nX_test = test_dataset.df.drop([\"churned\"], axis=1)\ny_test = test_dataset.df[[\"churned\"]].values.ravel()\n\nX_test.shape, y_test.shape\n\n((1000, 12), (1000,))\n\n\n\n\nDefine models\nWhile not exhaustive in our model search, we define four representative models that are suitable for binary classification with distinct strengths and weaknesses. We outline a high level discussion of these below:\n\n\n\n\n\n\n\n\n\nModel Type\nDescription\nStrengths\nWeaknesses\n\n\n\n\nLogistic Regression\nLinear model estimating probability of an instance belonging to a particular class\nHighly interpretable (through odds ratios) and computationally efficient\nAssumes linearity between features and log-odds of outcome, less robust against non-linear relationships and outliers\n\n\nSupport Vector Machines (SVC)\nModel that finds optimal hyperplane to seperate data points and maximize the margin between them\nRobust in high-dimensional spaces and handles non-linear relationships using kernel tricks\nComputationally expensive for large datasets, requires feature scaling, and less interpretable than linear models\n\n\nRandom Forest\nTree-based ensemble model that uses multiple decision trees trained on bootstrapped samples and random subset of features. Predictions are then aggregated\nReduces overfitting versus decision trees and robust to high-dimensional data and outliers\nLess interpretable than decision trees\n\n\nGradient Boosting\nTree-based ensemble model that builds trees sequentially with each tree correcting errors (missclassified instances) of the previous one (other implementations include XGBoost and LightGBM)\nHigh accuracy and performance especially on complex datasets\nProne to overfitting if not properly regularized and less interpretable than random forest\n\n\n\nWhile our classes are balanced in our dataset (both churned and not churned classes samples are similar), we will assess the models across the following metrics for completeness. Briefly:\n\nAccuracy: Total correct predictions / Total predictions\nPrecision: Total true positives / All positive predictions\nRecall: Total true positives / All actual positives\nF1 Score: Balanced score representing harmonic mean of precision and recall\nROC AUC: Summary metric quantifying the Area Under the (ROC) Curve; the ROC curve plots the true positive rate and false positive rate across different classification thresholds\n\nAccuracy and ROC AUC are most approrpiate given the balanced nature of our dataset, but for imbalanced datasets we would want to put more emphasis on Precision, Recall, and F1 Score. Additionally, the Precision-Recall Curve and it’s associated AUC would be preferable over the ROC AUC for imbalanced datasets.\nFor imbalanced datasets, a model with high accuracy overall could still yield poor performance for minority classes, making it a misleading metric. In this case, precision and recall provide a more useful measure for minority class prediction performance. For example, use:\n\nprecision if false positive cost is high (ie. did we predict the positive class correctly),\nrecall if false negative cost is high (ie. did we capture all of the positive class in our predictions), and\nF1 score if a balanced view on both is best.\n\n\n# Define models to compare\nsklearn_models = {\n    \"logistic_regression\": LogisticRegression(random_state=42, max_iter=1000),\n    \"svm\": SVC(probability=True, random_state=42),\n    \"random_forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"gradient_boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n}\n\n\n\nDefine preprocessing\nWhile not every model requires all the preprocessing steps below, we apply standard scaling on numeric variables (for Support Vector Classifier) and one hot encoding for categorical variables (all models).\nLogistic Regression does not require standard scaling, but benefits from it if we need to compare coefficients for relative importance of features and for fair regularization of feature coefficients.\nAdvanced gradient boosting libraries like XGBoost, LightGBM, and CatBoost have built-in support for categoricals which eschews the need for encoding altogether.\n\n# Define preprocessing steps for numerical and categorical features\nnumerical_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(drop='if_binary', sparse_output=False, handle_unknown='ignore')\n\npreprocessor = make_column_transformer(\n    (numerical_transformer, numerical_features),\n    (categorical_transformer, categorical_features),\n    remainder='drop',\n    verbose_feature_names_out=False\n)\n\n\n\nEvaluate models\nFinally, we assign our test dataset as the evaluation dataset for use in mlflow when calculating metrics for each of our model experiments. We iteratively train each model in a loop, including the preprocessing pipeline we defined above.\nSpecifically as we train each model, we ask mlflow to track the following for each model experiment run:\n\nRun name\nTags for the run\nSerialized model (persisted as .pkl)\nModel parameters\nModel signature (schema)\nMetrics against evaluation dataset\n\nThis allows us to have all experiments logged for future reference and comparison if we decide to conduct additional model experimentation. Otherwise, if we have a candidate model identified, we can further manage the model lifecycle with mlflow, including model registration, staging, production deployment, and continuous monitoring.\n\n# Create evaluation dataset\neval_data = X_test.copy()\neval_data = pd.DataFrame(eval_data, columns=X_test.columns.values)\neval_data[\"label\"] = y_test\n\n\n# Evaluate each model systematically\ncomparison_results = {}\n\nfor model_name, model in sklearn_models.items():\n    print(\"=\"*30)\n    print(f\"Evaluating {model_name}\")\n    print(\"=\"*30)\n    # Add preprocessing to model\n    print(\"Creating pipeline...\")\n    model_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('classifier', model)])\n\n    # Try the experiment\n    with mlflow.start_run(run_name=f\"eval_{model_name}\"):\n        print(\"Set tags...\")\n        # Use consistent tagging for easy filtering and organization\n        mlflow.set_tags(\n            {\n                \"model\": model_name,\n                \"dataset_version\": \"v1.0\",\n                \"feature_engineering\": \"standard\",\n                \"purpose\": \"baseline\",\n            }\n        )\n\n        # Log parameters\n        params = model_pipe.get_params()\n        params_subset = {key: params[key] for key in params.keys() if key.startswith('classifier')}\n        mlflow.log_params(params_subset)\n\n        print(\"Training model...\")\n        # Train model\n        model_pipe.fit(X_train, y_train)\n\n        print(\"Logging model...\")\n        # Log model\n        signature = infer_signature(X_train, model_pipe.predict(X_train))\n        model_info = mlflow.sklearn.log_model(model_pipe, name=model_name, signature=signature)\n        model_uri = mlflow.get_artifact_uri(model_name)\n\n        print(\"Evaluating model...\")\n        # Comprehensive evaluation with MLflow\n        result = mlflow.models.evaluate(\n            model_info.model_uri,\n            eval_data,\n            targets=\"label\",\n            model_type=\"classifier\",\n            evaluators=[\"classifier\"] # see mlflow.models.list_evaluators()\n        )\n\n        comparison_results[model_name] = result.metrics\n    \n        print(\"Logging metrics...\")\n        # Log key metrics for comparison\n        mlflow.log_metrics(\n            {\n                \"accuracy\": result.metrics[\"accuracy_score\"],\n                \"f1\": result.metrics[\"f1_score\"],\n                \"roc_auc\": result.metrics[\"roc_auc\"],\n                \"precision\": result.metrics[\"precision_score\"],\n                \"recall\": result.metrics[\"recall_score\"],\n            }\n        )\n\n==============================\nEvaluating logistic_regression\n==============================\nCreating pipeline...\nSet tags...\nTraining model...\nLogging model...\nEvaluating model...\n\n\n\n\n\nLogging metrics...\n🏃 View run eval_logistic_regression at: http://127.0.0.1:5000/#/experiments/888675540034408296/runs/639f6d9883ff4a9fb16f647e4a0488a9\n🧪 View experiment at: http://127.0.0.1:5000/#/experiments/888675540034408296\n==============================\nEvaluating svm\n==============================\nCreating pipeline...\nSet tags...\nTraining model...\nLogging model...\nEvaluating model...\n\n\n\n\n\nLogging metrics...\n🏃 View run eval_svm at: http://127.0.0.1:5000/#/experiments/888675540034408296/runs/d54df7cf231844bb92e4412c138a2634\n🧪 View experiment at: http://127.0.0.1:5000/#/experiments/888675540034408296\n==============================\nEvaluating random_forest\n==============================\nCreating pipeline...\nSet tags...\nTraining model...\nLogging model...\nEvaluating model...\n\n\n\n\n\nLogging metrics...\n🏃 View run eval_random_forest at: http://127.0.0.1:5000/#/experiments/888675540034408296/runs/337b2dbbb9e24fe2aab6c0b20776436b\n🧪 View experiment at: http://127.0.0.1:5000/#/experiments/888675540034408296\n==============================\nEvaluating gradient_boosting\n==============================\nCreating pipeline...\nSet tags...\nTraining model...\nLogging model...\nEvaluating model...\n\n\n\n\n\nLogging metrics...\n🏃 View run eval_gradient_boosting at: http://127.0.0.1:5000/#/experiments/888675540034408296/runs/e44db15a41e24ae0a66f82622b33b6c4\n🧪 View experiment at: http://127.0.0.1:5000/#/experiments/888675540034408296\n\n\n&lt;Figure size 1050x700 with 0 Axes&gt;",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 3: Modeling Strategy & Experiments"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part3_mlflow.html#evaluation-summary",
    "href": "projects/machine_learning/netflix_part3_mlflow.html#evaluation-summary",
    "title": "Netflix Part 3: Modeling Strategy & Experiments",
    "section": "Evaluation summary",
    "text": "Evaluation summary\nWith evaluation complete, we can retreive all logged information from the mlflow user interface.\nFor brevity we glean the most important metrics below and discuss other factors important when considering what model to move forward with. Specifically, we discuss some performance trade-offs to consider:\n\nPerformance goals\nIntepretability requirements\nComputational resources (training, inference, and memory)\nProbability calibration\n\n\nPerformance goals\nWhile all models performed very well generally with at or above 90% accuracy and F1 score, the tree-based ensemble methods produced the highest accuracy and F1 score by a sizeable margin above the SVM and Logistic Regression models (as high as 99%).\n\n\nIntepretability\nTraditionally, Logistic Regression provides the easiest interpretation through its coefficients. However with recent developments in explainable AI, namely SHAP (SHapley Additive exPlanations), more complex models can be made more interpretable using game theory to explain feature contributions to a model’s prediction locally and globally.\n\n\nComputational resources\nTraining time, inference time and memory usage are important factors depending on resources available and how frequently retraining and inference are conducted. More complex models like Random Forest, Gradient Boosting, and SVM can require more memory usage, training time, and inference time to varying degrees with larger datasets and the complexity of their specification (ie. number of trees, tree depth, number of features, kernel choice).\n\n\nProbability calibration\nIf the probability output of a model needs to be interpreted as a confidence level, then models need to be well calibrated. Calibration can be assessed using calibration curves (or reliability diagrams) that bins predicted probabilities of a model and plots the mean of each bin against the observed frequency. A perfectly calibrated model should have observed frequency (as fraction of positives) matching the predicted probability (of each bin); for example, the samples in the 0.2 predicted probability bin should have 20% actual positives within the bin. Generally, Logistic Regression and Random Forest are the best calibrated classifiers. For miscalibrated models, remediation can be attempted but requires added complexity through additional post-processing of model outputs.\n\n\nRecommendation\nBased on these factors, we recommend proceeding with the Gradient Boosting model, the most accurate model on evaluation data. Aside from accuracy, a few high points to reason for the relatively more complex model below:\n\nWhile predicting customer churn can occur at various frequencies, given Netflix’s subscription-based business and monthly billing cycles, a monthly frequency for retraining and batch inferencing would a be an appropriate starting point. This alleviates concerns for computational resources for more complex models despite Netflix likely having the compute capacity for higher frequencies if needed.\nInterpretability is important for actions to be taken when churn risk is identified, however as we noted earlier there are tools available for more complex models to achieve this.\nFinally, probability calibration is relevant for high stakes uses like predicting medical diagnoses where knowing level of risk is as important as the binary prediction. However, for deciding which customers to engage with in tailored marketing and pricing activities to reduce churn risk, the stakes are much lower and the probabilities less consequential. This could be revisited if more cost-intensive campaigns are required and more accurate lift charts are required to prioritize customers for maximum ROI.\n\nFinally, future rounds of experimentation can consider further reducing unimportant features to help with interpretability and performance (ie. using methods like variance thresholding or any other previously identified reasons during EDA). This applies for not just our chosen model but other models as well.\n\n\nComparison summary\n\n# Create comparison summary\ncomparison_df = pd.DataFrame(comparison_results).T\nprint(\"Model Comparison Summary:\")\nprint(comparison_df[[\"accuracy_score\", \"f1_score\", \"roc_auc\"]].round(3))\n\n# Identify best model\nbest_model = comparison_df[\"f1_score\"].idxmax()\nprint(f\"\\nBest model by F1 score: {best_model}\")\n\nModel Comparison Summary:\n                     accuracy_score  f1_score  roc_auc\nlogistic_regression           0.895     0.897    0.968\nsvm                           0.913     0.915    0.976\nrandom_forest                 0.978     0.978    0.998\ngradient_boosting             0.991     0.991    0.998\n\nBest model by F1 score: gradient_boosting\n\n\n\n\nArtifacts for best model\nFor additional assessment, artifacts for the best model can be examined below including the:\n\nConfusion matrix\nCalibration curve\nROC curve (Receiver Operating Characteristic curve)\nPrecision-recall curve (PR curve)\nLift Curve\n\nGiven the high accuracy and high F1 score of our chosen model and previous commentary regarding calibration curve importance, we forgo further discussion of these artifacts in favor of examining how to best interpret the model predictions in the last section of this project.\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\n\n# Retrieve run_id for best model from mlflow ui\nrun_id = 'a9b85b3c56e142ae895c9b56cb1f6072'\nrun = mlflow.get_run(run_id)\n\n# Download the artifacts locally\nlocal_artifact_path = mlflow.artifacts.download_artifacts(run_id=run_id)\nprint(f\"Artifacts downloaded to: {local_artifact_path}\")\n\n\n\n\nArtifacts downloaded to: /var/folders/9q/vx74myy94hdbfgg5kz3hj7nw0000gn/T/tmps6ee4u8n/\n\n\n\n# Get filenames for downloaded artifacts \nfile_names = os.listdir(local_artifact_path)\n\n# Load and display the image for each\nfor file in file_names:\n    file_path = local_artifact_path + file\n    img = mpimg.imread(file_path)\n    plt.imshow(img)\n    plt.axis('off') # Hide axes for a cleaner display\n    plt.show()",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 3: Modeling Strategy & Experiments"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part1_motivation.html",
    "href": "projects/machine_learning/netflix_part1_motivation.html",
    "title": "Netflix Part 1: Motivation",
    "section": "",
    "text": "Photo by DCL “650” on Unsplash\nThis is Part 1 of 4 in a project to build an explainable machine learning model to predict Netflix customer churn.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 1: Motivation"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part1_motivation.html#business-context",
    "href": "projects/machine_learning/netflix_part1_motivation.html#business-context",
    "title": "Netflix Part 1: Motivation",
    "section": "Business context",
    "text": "Business context\nAs the undisputed pioneer of the streaming age, Netflix established the core principles of the over-the-top (OTT) media model. Its groundbreaking success fundamentally disrupted the entertainment industry, triggering a monumental shift as consumers embraced its subscription-based, on-demand service.\nThe company currently stands as one of the world’s leading entertainment services, boasting approximately 302 million paid memberships across over 190 countries as of December 31, 20241.\nHowever, the market for entertainment video is intensely competitive and subject to rapid change. Netflix faces a broad spectrum of competitors vying for consumers’ leisure time. These include:\n\nTraditional linear television\nOther streaming entertainment providers\nVideo gaming providers\nUser-generated content platforms\nBroader sources of entertainment\n\nIn this highly competitive and dynamic environment, managing customer churn (member cancellations) has become critically important for Netflix. The company explicitly seeks to “win moments of truth” with its members1, striving for them to choose Netflix in their free time. Several factors underscore the heightened importance of churn management:\n\nFixed Content Costs and Profitability Impact: Netflix’s content costs are largely fixed in nature\nDiverse Reasons for Cancellation\nImpact of Pricing, Plan, and Usage Enforcement\nContent Investment Risk and Member Satisfaction\nWord-of-Mouth and Reputation\nIntensifying Competition\n\nTo put a number on managing churn with regards to the first factor: Netflix’s fixed content costs of nearly $23.25 billion needs about 166 million subscribers2 (or roughly half its customer base) just to cover those costs. This means it must keep its annual churn rate below 46% to break even, and do substantially more to maintain profitability, all other costs ignored.\nAs such, a key pillar of managing churn is the ability to proactively identify at-risk subscribers and understand the reasons they might leave. This allows Netflix to address each one with tailored marketing and pricing strategies, which is essential for sustaining its market leadership, meeting growth targets, and ensuring long-term profitability in a competitive landscape.\n1 Source: Netflix 2024 Annual Report 2 Assuming $140 annualized ARM (average monthly revenue per paying membership)",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 1: Motivation"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part1_motivation.html#executive-summary",
    "href": "projects/machine_learning/netflix_part1_motivation.html#executive-summary",
    "title": "Netflix Part 1: Motivation",
    "section": "Executive summary",
    "text": "Executive summary\nFrom our rigourous process that are detailed in the subsequent sections, we achieved the following which can help reduce Netflix’s churn rate to as low as 1%1:\n\nDevelop a strong predictive model that provides 99% accuracy in predicting churn (using Gradient Boosting), with an auxillary model that provides 98% if confidence levels are important (using Random Forest)\nCreate the capability to explain both the global model feature importances, and more importantly, ranked reasons for why an individual subscriber is predicted to churn or not (applicable to either models we developed)\n\nIn tandem these solutions can help form the foundations of Netflix’s ability to address at-risk subscribers with tailored marketing and pricing strategies.\nNext steps\n\nModel deployment to production environment and ensure inferencing API is available to other applications and systems for tailored subscriber churn interventions\nModel monitoring to track performance in production and identify issues like model drift, then retrain and recalibrate the model, archiving previous versions as required\n\nFuture opportunities\n\nThe elephant in the room - real data can clearly improve upon synthetic data limitations with regards to generalizability\nWhile the data available exhibits a useful cross section of churn-related features across product usage, payment behaviour, demographics, and contextual information, it could be worth understanding the customer experience more deeply in the following ways\nInclude capturing data that helps a) understand customer values and benefits from product use (ie. genre specific watch hours, viewing patterns and trend, catalogue availability and trend) and b) understand how well the product helps to deliver said benefits (ie. quality of experience metrics, customer satisfaction interactions and survey responses, product usage details such as feature engagement and time in app)\nWith more complex data sets, other complex models (neural networks) and further hyperparameter tuning may be warranted for future models\n\n1Predicated on effective churn risk reduction interventions",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 1: Motivation"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part1_motivation.html#methodology",
    "href": "projects/machine_learning/netflix_part1_motivation.html#methodology",
    "title": "Netflix Part 1: Motivation",
    "section": "Methodology",
    "text": "Methodology\nTo solve this problem, we employ machine learning approaches to create a model that is able to leverage subscriber engagement and demographic data to predict churn likelihood. This is technically achieved through a three-step approach:\n\nData understanding and EDA insights using pandas, numpy, plotly, and statsmodels (see part 2)\nModeling strategy and experiments using scikit-learn and mlflow (see part 3)\nModel interpretation and insights using shap (see part 4)",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 1: Motivation"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part1_motivation.html#data-background",
    "href": "projects/machine_learning/netflix_part1_motivation.html#data-background",
    "title": "Netflix Part 1: Motivation",
    "section": "Data background",
    "text": "Data background\nWe leverage a synthetic sample of 5,000 subscribers (see source) where each unit of observation captures a single subscriber’s information across demographic and engagement attributes that reasonably resemble data that Netflix likely has available through their day-to-day operations. Importantly, it also includes their churn status.\nWhile the data presents a more-dire-than-reality view on Netflix churn (the classes are relatively balanced between churned and not churned at 50/50), we could attribute this to a hypothetical preprocessing strategy conducted by upstream data engineering teams, where Synthetic Minority Over-sampling Technique (or SMOTE) has been applied to over-sample the churned subscribers (typically the minority class). In reality, if we had it any other way, SMOTE should be applied after train/test split to prevent data leakage and allow for more reliable evaluations against real-world distributions.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 1: Motivation"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "",
    "text": "Photo by Tomás Evaristo on Unsplash\nThis is Part 4 of 4 in a project to build an explainable machine learning model to predict Netflix customer churn.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#overview",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#overview",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "Overview",
    "text": "Overview\nIn this section, we take the best model from our model exprimentation and derive meaningful interpretation of it through the use of SHAP (SHapley Additive exPlanations) from the shap library, an approach that uses game theory to explain feature contributions to a model’s prediction locally and globally.",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#imports",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#imports",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "Imports",
    "text": "Imports\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# Import mlflow and shap\nimport mlflow\nimport shap",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#test-data-preperation",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#test-data-preperation",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "Test data preperation",
    "text": "Test data preperation\nWe will expediently load the same data from our model experiment section, using the test data once more for predictions and model intepretation.\n\n# Load the data\nproject_path = os.getcwd()\ndf = pd.read_csv(project_path + \"/Data/netflix_customer_churn.csv\")\n\n# Create column name lists for reference\nnumerical_features = ['age', 'watch_hours', 'last_login_days', 'avg_watch_time_per_day']\ncategorical_features = ['gender', 'subscription_type', 'region', 'device', 'payment_method', 'favorite_genre', 'monthly_fee',  'number_of_profiles']\ntarget = ['churned']\n\n# Make all categoricals of string data type\nfor category in categorical_features:\n    df[category] = df[category].astype(\"string\")\n\n# Make all numerical of float data type (be wary of precision loss for large integers)\nfor numerical in numerical_features:\n    df[numerical] = df[numerical].astype(\"float\")\n\n# Drop target and non-feature\nX = df.drop([\"customer_id\", \"churned\"], axis=1)\n\n# Split into 80% training and 20% testing\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape\n\n((4000, 12), (1000, 12))",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#load-the-best-model",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#load-the-best-model",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "Load the best model",
    "text": "Load the best model\nSince we already have our chosen trained and serialized model, we load from our previous experiment run. If we had decided this was the model to deploy, we would register this model with versioning and access the model differently.\n\n# Connect MLflow session to local server\nmlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n\n\n# Use the model_id from best model via mlflow ui\nmodel_id = \"m-f7686021a61e45dabd92cae285b65d97\"\n# Load the model using the appropriate flavor-specific load method\nloaded_model = mlflow.sklearn.load_model(f\"models:/{model_id}\")",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-setup",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-setup",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "SHAP setup",
    "text": "SHAP setup\nBased on game theory, SHAP (SHapley Additive exPlanations) values help show how each feature affects a model’s final prediction, the significance of each feature compared to others, and the model’s reliance on the interaction between features. Positive values indicate positive impact to the prediction and negative values indicate negative impact. The magnitude indicates the strength of the effect.\nMost importantly, SHAP values provide useful properties best summarized by the following excerpt from DataCamp:\n\nAdditivity: SHAP values are additive, which means that the contribution of each feature to the final prediction can be computed independently and then summed up. This property allows for efficient computation of SHAP values, even for high-dimensional datasets.\nLocal accuracy: SHAP values add up to the difference between the expected model output and the actual output for a given input. This means that SHAP values provide an accurate and local interpretation of the model’s prediction for a given input.\nMissingness: SHAP values are zero for missing or irrelevant features for a prediction. This makes SHAP values robust to missing data and ensures that irrelevant features do not distort the interpretation.\nConsistency: SHAP values do not change when the model changes unless the contribution of a feature changes. This means that SHAP values provide a consistent interpretation of the model’s behavior, even when the model architecture or parameters change.\n\nAdditional background and paper details can be found on GitHub.\n\nCreate explainer\nTypically we can directly pass a model to shap.Explainer. However our saved model is defined within a preprocessing pipeline, meaning we need to take three a step approach: 1) extract the pipeline, 2) extract the classification model, and 3) transform the data independently using the extracted pipeline and provide this along with the extracted model to shap.Explainer.\nWe use shap.TreeExplainer below (since we know we are providing a tree-based model).\n\n# Extract from our model pipeline\nextracted_preproc = loaded_model['preprocessor'] \nextracted_model = loaded_model['classifier']\n\n# Preprocess observations\nsample = extracted_preproc.transform(X_test.sample(100, random_state=42))\nobservations = extracted_preproc.transform(X_test)\n\n# Recover the feature names for proper labels in plots\nobservations = pd.DataFrame(observations, columns=extracted_preproc.get_feature_names_out())\n\nBefore going further, it is important to understand that Gradient Boosting and many tree-based classification models are typically optimized against a loss function and not the transformed probabilities, so their raw outputs would represent log-odds. shap by default will compute SHAP values with respect to this raw output ie. log-odds.\nTo improve interpretability log odds can be transformed using the inverse logit function \\(1 / (1 + exp(-x))\\)\nIn the shap library there are two ways to approximate this: 1. Within TreeExplainer the option exists to set model_output=\"probability\" to get SHAP values in probability space 2. Some plots provide link=\"logit\" to transform the outputs to represent a probability\nWe will apply the first option below.\n\n# Create explainer with model and optional sample data\nexplainer = shap.TreeExplainer(extracted_model, sample, model_output=\"probability\")\n\n# Calculate SHAP values\nshap_values = explainer(observations) # new style",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-global-interpretation",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-global-interpretation",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "SHAP global interpretation",
    "text": "SHAP global interpretation\nThe following two plots show different views into global feature importance.\n\nBar plot\nThe following provides a global feature importance plot, where SHAP values for each feature are the absolute mean for all individual predictions. The default max_display value helps combine the less significant features into a last group that is a sum of SHAP values for the remaining features.\nThe importances are closely aligned with what we found during EDA, including avg_watch_time_per_day, watch_hours, and last_login_days.\n\nshap.plots.bar(shap_values)\n\n\n\n\n\n\n\n\n\n\nBeeswarm plot\nThe following plot provides a more detailed view by plotting SHAP values for each feature along the x-axis in a beeswarm manner. Each feature value is further colored from low to high based on each feature’s range of values.\nIn addition to seeing the importance of each feature, this enables us to see how a feature’s value (color) influences the magnitude and direction of it’s impact on the model’s final prediction (location along the x-axis).\n\nFor example, the clearest display of this is in last_login_days, where clearly high values (ie. long lapses in login days) influence predictions positively (ie. more likely to churn), as illustrated by the swarm of red on the right hand side of the plot.\nAn example in the opposite direction is watch_hours, where high values (ie. many hours watched) influence predictions negatively (ie. less likely to churn); however the relationship is sometimes less definitive as illustrated by a mixture of red, blue, and purple around smaller positive and negative SHAP values.\n\n\nshap.plots.beeswarm(shap_values)",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-local-interpretation",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-local-interpretation",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "SHAP local interpretation",
    "text": "SHAP local interpretation\nThe following three plots below show how SHAP values can used to explain each feature’s contribution to an individual (local) prediction by our model.\nWe will follow the first sample subscriber in the test dataset (index 0) in the subsequent plots. As a reference, note that this subscriber is predicted to churn with a 53% probability1.\n1Remember that the probability calibration is imperfect for our model so this should be interpreted with caution\n\n# Choose the index in the test data to predict\nidx = 0\n# Get the feature values for a single observation\nX_values = observations.iloc[[idx]].values\n# Get prediction and probability for the single observation\npred, pred_proba = extracted_model.predict(X_values), extracted_model.predict_proba(X_values)\n# Print the prediction\nprint(f\"Prediction @ Index {idx}: {'Churn' if pred == 1 else 'Not Churn'} with {pred_proba[0][1]:.1%} probability\")\n\nPrediction @ Index 0: Churn with 53.1% probability\n\n\n\nWaterfall plot\nThe waterfall plot description in the API reference provides an informative overview:\n\nThe SHAP value of a feature represents the impact of the evidence provided by that feature on the model’s output. The waterfall plot is designed to visually display how the SHAP values (evidence) of each feature move the model output from our prior expectation under the background data distribution, to the final model prediction given the evidence of all the features.\nFeatures are sorted by the magnitude of their SHAP values with the smallest magnitude features grouped together at the bottom of the plot when the number of features in the models exceeds the max_display parameter.\n\nIn context, the waterfall plot of our sample subscriber tells us that our model:\n\nExpects the prior expectation (probability of churn) for the average subscriber to be 0.41\nSuggests this subscriber will churn largely because: the subscriber pays with a gift card, has lower than average daily watch hours, and is on a basic subscription plan\nConsidered that the subscriber’s last login days, watch hours, and number of profiles as reasons the subscriber might not churn, however these did not outweigh the other drivers for churn.\n\nAside from making predictions more interpretable, the specific feature importances leading to the prediction provide valuable insights into what actions Netflix may want to pursue to reduce the churn risk for this subscriber.\nIn this example, given that the payment method is the largest driver, Netflix can decide to encourage this user to add a credit card as a payment instead (with incentives such as a six month free upgrade to a Premium subscription plan) in order to improve customer stickiness.\nNote that this begins to tread into causal relationship territory where understanding confounding factors may be important. Either drawing from prior business knowledge or further causal modeling may be warranted to further strengthen such recommendations.\n\nshap.plots.waterfall(shap_values[0])\n\n\n\n\n\n\n\n\n\n\nForce plot\nA force plot provides the same information as the waterfall plot above but in a more compact graph.\n\nshap.plots.force(base_value=explainer.expected_value, \n                shap_values=shap_values.values[0], \n                features=observations.iloc[0].round(2),\n                feature_names=shap_values.feature_names,\n                contribution_threshold=0.15,\n                matplotlib=True)\n\n\n\n\n\n\n\n\n\n\nDecision plot\nSimilarily, a decision plot takes the compactness further and summarizes the SHAP values into a single line that begins at the prior expected value (bottom) and ends at predicted value (top). This is effective for comparing two or more prediction at the same time.\n\nshap.plots.decision(base_value=explainer.expected_value,\n                    shap_values=shap_values.values[0],\n                    features=observations.iloc[0],\n                    feature_names=shap_values.feature_names,\n                    highlight=[0])",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-comparing-multiple-predictions",
    "href": "projects/machine_learning/netflix_part4_shap_explainer.html#shap-comparing-multiple-predictions",
    "title": "Netflix Part 4: Model Interpretation & Insights",
    "section": "SHAP comparing multiple predictions",
    "text": "SHAP comparing multiple predictions\n\nDecision plot\nWhen multiple predictions are plotted, the feature values are not plotted but we are instead offered a more comparative and diagnostic view of how features impact the various plotted predictions.\nThe decision plot below shows the decision paths for predictions on the first ten subscribers in our test dataset. The first subscriber we examined earlier is highlighted with a bold dotted line.\nWithout the feature values it is harder to discern why the SHAP values shift one way versus another between predictions.\nHowever, what we can observe is the same abnormal relationship between watch_hours, last_login_days, and avg_watch_time_per_day as we saw during EDA for the predictions in solid purple.\n\nlast_login_days and avg_watch_time_per_day clearly move the prediction closer to a probability for churn presumably due to long lapses in login days and low daily watch hours,\nhowever, watch_hours brings the prediction back to low probability for churn, presumably due to high overall watch hours.\n\n\nshap.plots.decision(base_value=explainer.expected_value,\n                    shap_values=shap_values.values[0:10], features=shap_values.feature_names,\n                    highlight=[0])",
    "crumbs": [
      "Projects",
      "Machine Learning",
      "Netflix Part 4: Model Interpretation & Insights"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Featured Projects",
    "section": "",
    "text": "A curated collection of projects\n\n\n\n\n\n\n\n\nNetflix Part 1: Motivation\n\n5 min\n\n\n\n\n\nDave Li\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nNetflix Part 3: Modeling Strategy & Experiments\n\n9 min\n\n\n\n\n\nDave Li\n\n\nAug 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nNetflix Part 4: Model Interpretation & Insights\n\n8 min\n\n\n\n\n\nDave Li\n\n\nAug 29, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects",
      "Featured Projects"
    ]
  }
]